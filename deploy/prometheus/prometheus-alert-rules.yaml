---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prometheus
    role: alert-rules
  name: prometheus-clickhouse-operator-rules
spec:
  groups:
    - name: ClickHouseOperatorRules
      rules:
        - alert: ClickHouseMetricsExporterDown
          expr: up{job='clickhouse-operator-metrics'} == 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.pod }}"
            summary: "metrics-exporter possible down"
            description: |-
              `metrics-exporter` not sent data more than 1 minutes.
              Please check instance status
              ```kubectl logs -n {{ $labels.namespace }} {{ $labels.pod }} -c metrics-exporter -f```

        - alert: ClickHouseServerDown
          expr: chi_clickhouse_metric_fetch_errors > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "clickhouse-server possible down"
            description: |-
              `metrics-exporter` failed metrics fetch `{{ $labels.fetch_type }}`.
              Please check instance status
              ```kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1)```

        - alert: ClickHouseServerRestartRecently
          expr: chi_clickhouse_metric_Uptime > 1 < 180
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "clickhouse-server started recently"
            description: |-
              `chi_clickhouse_metric_Uptime` = {{ with printf "chi_clickhouse_metric_Uptime{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} seconds {{ end }}
              `clickhouse-server` process has been start less than 3 minutes ago.
              Look to previous ClickHouse pod log to investigate restart reason
              ```
              kubectl logs -n {{ $labels.exported_namespace }} $( echo {{ $labels.hostname }} | cut -d '.' -f 1)-0 --previous```
              ```

        - alert: ClickHouseDNSErrors
          expr: increase(chi_clickhouse_event_DNSError[1m]) > 0 or increase(chi_clickhouse_event_NetworkErrors[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "DNS errors occurred"
            description: |-
              `increase(chi_clickhouse_event_DNSError[1m])` = {{ with printf "increase(chi_clickhouse_event_DNSError{hostname='%s',exported_namespace='%s'}[1m]) or increase(chi_clickhouse_event_NetworkErrors{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} errors{{ end }}
              Please check DNS settings in `/etc/resolve.conf` and `<remote_servers>` part of `/etc/clickhouse-server/`
              See documentation:
              - https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-remote-servers
              - https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-disable-internal-dns-cache
              - https://clickhouse.tech/docs/en/query_language/system/#query_language-system-drop-dns-cache

        - alert: ClickHouseDistributedFilesToInsertHigh
          expr: chi_clickhouse_metric_DistributedFilesToInsert > 50
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "clickhouse-server have Distributed Files to Insert > 50"
            description: |-
              `chi_clickhouse_metric_DistributedFilesToInsert` = {{ with printf "chi_clickhouse_metric_DistributedFilesToInsert{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} files{{ end }}
              `clickhouse-server` have too much files which not insert to `*MergeTree` tables via `Distributed` table engine
              Check not synced .bin files via ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- ls -la /var/lib/clickhouse/data/*/*/*/*.bin```

              Also, check documentation:
              https://clickhouse.tech/docs/en/operations/table_engines/distributed/
              When you insert data to `Distributed` table.
              Data is written to target `*MergreTree` tables asynchronously.
              When inserted in the table, the data block is just written to the local file system.
              The data is sent to the remote servers in the background as soon as possible.
              The period for sending data is managed by the `distributed_directory_monitor_sleep_time_ms` and `distributed_directory_monitor_max_sleep_time_ms` settings.
              The Distributed engine sends each file with inserted data separately, but you can enable batch sending of files with the `distributed_directory_monitor_batch_inserts` setting

              Also, you can manage distributed tables:
              https://clickhouse.tech/docs/en/sql-reference/statements/system/#query-language-system-distributed

        - alert: ClickHouseDistributedConnectionExceptions
          expr: increase(chi_clickhouse_event_DistributedConnectionFailTry[1m]) > 0 or increase(chi_clickhouse_event_DistributedConnectionFailAtAll[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Distributed connections fails occurred"
            description: |-
              `increase(chi_clickhouse_event_DistributedConnectionFailTry[1m])` = {{ with printf "increase(chi_clickhouse_event_DistributedConnectionFailTry{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} errors{{ end }}
              `increase(chi_clickhouse_event_DistributedConnectionFailAtAll[1m])` = {{ with printf "increase(chi_clickhouse_event_DistributedConnectionFailAtAll{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} errors{{ end }}

              Please, check communications between clickhouse server and host `remote_servers` in `/etc/clickhouse-server/`
              https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-remote-servers

              Also, you can check logs:
              ```kubectl logs -n {{ $labels.exported_namespace }} $( echo {{ $labels.hostname }} | cut -d '.' -f 1)-0 -f```

        - alert: ClickHouseRejectedInsert
          expr: increase(chi_clickhouse_event_RejectedInserts[1m]) > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Rejected INSERT queries occurred"
            description: |-
              `increase(chi_clickhouse_event_RejectedInserts[1m])` = {{ with printf "increase(chi_clickhouse_event_RejectedInserts{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} queries{{ end }}
              `clickhouse-server` have INSERT queries that are rejected due to high number of active data parts for partition in a MergeTree, please decrease INSERT frequency
              MergeTreeArchitecture
              https://clickhouse.tech/docs/en/development/architecture/#merge-tree
              system.parts_log
              https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-part-log
              system.merge_tree_settings
              https://clickhouse.tech/docs/en/operations/system-tables/#system-merge_tree_settings


        - alert: ClickHouseDelayedInsertThrottling
          expr: increase(chi_clickhouse_event_DelayedInserts[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Delayed INSERT queries occurred"
            description: |-
              `increase(chi_clickhouse_event_DelayedInserts[1m])` = {{ with printf "increase(chi_clickhouse_event_DelayedInserts{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} queries{{ end }}
              `clickhouse-server` have INSERT queries that are throttled due to high number of active data parts for partition in a MergeTree, please decrease INSERT frequency
              https://clickhouse.tech/docs/en/development/architecture/#merge-tree

        - alert: ClickHouseMaxPartCountForPartition
          expr: chi_clickhouse_metric_MaxPartCountForPartition > 100
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Max parts per partition > 100"
            description: |-
              `chi_clickhouse_metric_MaxPartCountForPartition` = {{ with printf "chi_clickhouse_metric_MaxPartCountForPartition{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} parts{{ end }}
              `clickhouse-server` have too many parts in one partition.
              Clickhouse MergeTree table engine split each INSERT query to partitions (PARTITION BY expression)
              and add one or more PARTS per INSERT inside each partition, after that background merge process run,
              and when you have too much unmerged parts inside partition,
              SELECT queries performance can significate degrade, so clickhouse try delay or reject INSERT

        - alert: ClickHouseLowInsertedRowsPerQuery
          expr: increase(chi_clickhouse_event_InsertQuery[1m]) > 0 and (increase(chi_clickhouse_event_InsertedRows[1m]) / increase(chi_clickhouse_event_InsertQuery[1m]) <= 1000)
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "please increase inserted rows per INSERT query"
            description: |-
              `increase(chi_clickhouse_event_InsertedRows[1m]) / increase(chi_clickhouse_event_InsertQuery[1m])` = {{ with printf "increase(chi_clickhouse_event_InsertedRows{hostname='%s',exported_namespace='%s'}[1m]) / increase(chi_clickhouse_event_InsertQuery{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} rows per query{{ end }}
              `clickhouse-server` have low insert speed.
              https://clickhouse.tech/docs/en/introduction/performance/#performance-when-inserting-data
              Clickhouse team recommends inserting data in packets of at least 1000 rows or no more than a single request per second.

              Please use Buffer table
              https://clickhouse.tech/docs/en/operations/table_engines/buffer/
              or
              https://github.com/nikepan/clickhouse-bulk
              or
              https://github.com/VKCOM/kittenhouse

        - alert: ClickHouseLongestRunningQuery
          expr: chi_clickhouse_metric_LongestRunningQuery > 600
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Long running queries occurred"
            description: |-
              `clickhouse-server` have queries that running more than `chi_clickhouse_metric_LongestRunningQuery` = {{ with printf "chi_clickhouse_metric_LongestRunningQuery{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} seconds{{ end }}
              try look to system.processes with long queries
              https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-processes
              ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- clickhouse-client -q "SELECT * FROM system.processes WHERE elapsed >= 600 FORMAT Vertical" | less```

        - alert: ClickHouseQueryPreempted
          expr: chi_clickhouse_metric_QueryPreempted > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Preempted queries occurred"
            description: |-
              `clickhouse-server` have `chi_clickhouse_metric_QueryPreempted` = {{ with printf "chi_clickhouse_metric_QueryPreempted{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} queries{{ end }}
              It mean queries that are stopped and waiting due to 'priority' setting.
              try look to system.processes
              https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-processes
              ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- clickhouse-client -q "SELECT * FROM system.processes FORMAT Vertical" | less```


        - alert: ClickHouseReadonlyReplica
          expr: chi_clickhouse_metric_ReadonlyReplica > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ReadOnly replica occurred"
            description: |-
              `chi_clickhouse_metric_ReadonlyReplica` = {{ with printf "chi_clickhouse_metric_ReadonlyReplica{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} replicas{{ end }}
              `clickhouse-server` have ReplicatedMergeTree tables that are currently in readonly state due to re-initialization after ZooKeeper session loss or due to startup without ZooKeeper configured.
              Please check following things:
              - kubenetes nodes have free enough RAM and Disk via `kubectl top node`
              - status of clickhouse-server pods ```kubectl describe -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1)```
              - connection between clickhouse-server pods and zookeeper ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- clickhouse-client -q "SELECT * FROM system.zookeeper WHERE path='/' FORMAT Vertical"```
              - connection between clickhouse-server pods via kubernetes services ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- clickhouse-client -q "SELECT host_name, errors_count FROM system.clusters WHERE errors_count > 0 FORMAT PrettyCompactMonoBlock"```
              - status of PersistentVolumeClaims for pods ```kubectl get pvc -n {{ $labels.exported_namespace }}```
              Also read documentation:
              https://clickhouse.tech/docs/en/operations/table_engines/replication/#recovery-after-failures

        - alert: ClickHouseReplicasMaxAbsoluteDelay
          expr: chi_clickhouse_metric_ReplicasMaxAbsoluteDelay > 300
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Replication Lag more 300s occurred"
            description: |-
              `clickhouse-server` have replication lag `chi_clickhouse_metric_ReplicasMaxAbsoluteDelay` = {{ with printf "chi_clickhouse_metric_ReplicasMaxAbsoluteDelay{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} seconds{{ end }}.
              When replica have too much lag, it can be skipped from Distributed SELECT Queries without errors and you will have wrong query results.
              Check free disk space disks and network connection between clickhouse pod and zookeeper on monitored clickhouse-server pods

              Also read documentation:
              - https://clickhouse.tech/docs/en/operations/table_engines/replication/#recovery-after-failures
              - https://clickhouse.tech/docs/en/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries

        - alert: ClickHouseTooManyConnections
          expr: chi_clickhouse_metric_HTTPConnection + chi_clickhouse_metric_TCPConnection + chi_clickhouse_metric_MySQLConnection > 100
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Total connections > 100"
            description: |-
              `chi_clickhouse_metric_HTTPConnection` = {{ with printf "chi_clickhouse_metric_HTTPConnection{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} connections{{ end }}
              `chi_clickhouse_metric_TCPConnection` = {{ with printf "chi_clickhouse_metric_TCPConnection{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} connections{{ end }}
              `chi_clickhouse_metric_MySQLConnection` = {{ with printf "chi_clickhouse_metric_MySQLConnection{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value }} connections{{ end }}

              `clickhouse-server` have many open connections.
              The ClickHouse is adapted to run not a very large number of parallel SQL requests, not every HTTP/TCP(Native)/MySQL protocol connection means a running SQL request, but a large number of open connections can cause a spike in sudden SQL requests, resulting in performance degradation.

              Also read documentation:
              - https://clickhouse.tech/docs/en/operations/server_settings/settings/#max-concurrent-queries


        - alert: ClickHouseTooMuchRunningQueries
          expr: chi_clickhouse_metric_Query > 80
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Too much running queries"
            description: |-
              `clickhouse-server` have {{ with printf "chi_clickhouse_metric_Query{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.0f" }} running queries{{ end }}
              Please analyze your workload.
              Each concurrent SELECT query use memory in JOINs use CPU for running aggregation function and can read lot of data from disk when scan parts in partitions and utilize disk I/O.
              Each concurrent INSERT query, allocate around 1MB per each column in an inserted table and utilize disk I/O.

              Look at following documentation parts:
              - https://clickhouse.tech/docs/en/operations/settings/query_complexity/
              - https://clickhouse.tech/docs/en/operations/quotas/
              - https://clickhouse.tech/docs/en/operations/server_settings/settings/#max-concurrent-queries
              - https://clickhouse.tech/docs/en/operations/system-tables/#system_tables-query_log

        - alert: ClickHouseSystemSettingsChanged
          expr: delta(chi_clickhouse_metric_ChangedSettingsHash[5m]) != 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "`system.settings` changed"
            description: |-
              `clickhouse-server` changed `chi_clickhouse_metric_ChangedSettingsHash` = {{ with printf "chi_clickhouse_metric_ChangedSettingsHash{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}

        - alert: ClickHouseVersionChanged
          expr: delta(chi_clickhouse_metric_VersionInteger[5m]) != 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ClickHouse version changed"
            description: |-
              `clickhouse-server` changed `chi_clickhouse_metric_VersionInteger` = {{ with printf "chi_clickhouse_metric_VersionInteger{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}

        - alert: ClickHouseZooKeeperHardwareExceptions
          expr: increase(chi_clickhouse_event_ZooKeeperHardwareExceptions[1m]) > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ZooKeeperHardwareExceptions > 1"
            description: |-
              `increase(chi_clickhouse_event_ZooKeeperHardwareExceptions[1m])` = {{ with printf "increase(chi_clickhouse_event_ZooKeeperHardwareExceptions{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }} exceptions{{ end }}
              `clickhouse-server` have unexpected Network errors and similar with communitation with Zookeeper.
              Clickhouse should reinitialize ZooKeeper session in case of these errors.

        - alert: ClickHouseZooKeeperSession
          expr: chi_clickhouse_metric_ZooKeeperSession > 1
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "ZooKeeperSession > 1"
            description: |-
              `chi_clickhouse_metric_ZooKeeperSession` = {{ with printf "chi_clickhouse_metric_ZooKeeperSession{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.0f" }} sessions{{ end }}
              Number of sessions (connections) from `clickhouse-server` to `ZooKeeper` shall be no more than one,
              because using more than one connection to ZooKeeper may lead to bugs due to lack of linearizability (stale reads)
              that ZooKeeper consistency model allows.

        - alert: ClickHouseDiskUsage
          expr: (chi_clickhouse_metric_DiskDataBytes / (chi_clickhouse_metric_DiskFreeBytes + chi_clickhouse_metric_DiskDataBytes)) > 0.8
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Disk Usage in `&lt;yandex&gt;&lt;path&gt;` > 80%"
            description: |-
              data size: {{ with printf "chi_clickhouse_metric_DiskDataBytes{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | humanize1024 }}B {{ end }}
              disk free {{ with printf "chi_clickhouse_metric_DiskFreeBytes{hostname='%s',exported_namespace='%s'}" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | humanize1024 }}B {{ end }}
              currently k8s doesn't support resize of Persistent Volumes, but you can try add another volume to existing pod with restart pod
              please read documentation:
              - https://github.com/Altinity/clickhouse-operator/blob/master/docs/storage.md
              - https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-multiple-volumes
              - https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-table-ttl

# not well tested alerts which can't be triggered on e2e tests
        - alert: ClickHouseReplicatedPartChecksFailed
          expr: increase(chi_clickhouse_event_ReplicatedPartChecksFailed[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReplicatedPartCheckFailed"
            description: |-
              `increase(chi_clickhouse_event_ReplicatedPartChecksFailed[1m])` = {{ with printf "increase(chi_clickhouse_event_ReplicatedPartChecksFailed{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase ReplicatedPartCheckFailed in `system.events` table.
              Please check logs on clickhouse-server pods ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- cat /var/log/clickhouse-server/*.err.log | less```

        - alert: ClickHouseReplicatedPartFailedFetches
          expr: increase(chi_clickhouse_event_ReplicatedPartFailedFetches[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReplicatedPartFailedFetches"
            description: |-
              `increase(chi_clickhouse_event_ReplicatedPartFailedFetches[1m])` = {{ with printf "increase(chi_clickhouse_event_ReplicatedPartFailedFetches{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase ReplicatedPartFailedFetches in `system.events` table.
              It mean server was failed to download data part from replica of a ReplicatedMergeTree table.
              Please check following things:
              - connections between clickhouse-server pod and his replicas (see remote_server section in /etc/clickhouse-server/)
              - logs on clickhouse-server pods ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- cat /var/log/clickhouse-server/*.err.log | less```

        - alert: ClickHouseReplicatedDataLoss
          expr: increase(chi_clickhouse_event_ReplicatedDataLoss[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReplicatedDataLoss"
            description: |-
              `increase(chi_clickhouse_event_ReplicatedDataLoss[1m])` = {{ with printf "increase(chi_clickhouse_event_ReplicatedDataLoss{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase ReplicatedDataLoss in `system.events` table.
              It mean data part that server wanted doesn't exist on any replica (even on replicas that are offline right now).
              That data parts are definitely lost. This is normal due to asynchronous replication (if quorum inserts were not enabled),
              when the replica on which the data part was written was failed and when it became online after fail
              it doesn't contain that data part.

              Please check logs on clickhouse-server pods ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- cat /var/log/clickhouse-server/*.err.log | less```

        - alert: ClickHouseStorageBufferErrorOnFlush
          expr: increase(chi_clickhouse_event_StorageBufferErrorOnFlush[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased StorageBufferErrorOnFlush"
            description: |-
              `increase(chi_clickhouse_event_StorageBufferErrorOnFlush[1m])` = {{ with printf "increase(chi_clickhouse_event_StorageBufferErrorOnFlush{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase StorageBufferErrorOnFlush in `system.events` table.
              It mean something went wrong when clickhouse-server try to flush memory buffers to disk.

              Please check following things:
              - check disks free space and hardware failures
              - logs on clickhouse-server pods ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- cat /var/log/clickhouse-server/*.err.log | less```

        - alert: ClickHouseDataAfterMergeDiffersFromReplica
          expr: increase(chi_clickhouse_event_DataAfterMergeDiffersFromReplica[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased DataAfterMergeDiffersFromReplica"
            description: |-
              `increase(chi_clickhouse_event_DataAfterMergeDiffersFromReplica[1m])` = {{ with printf "increase(chi_clickhouse_event_DataAfterMergeDiffersFromReplica{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase DataAfterMergeDiffersFromReplica in `system.events` table.
              It mean Data after merge is not byte-identical to data on another replicas.
              There could be several reasons:
              - Using newer version of compression library after server update.
              - Using another compression method.
              - Non-deterministic compression algorithm (highly unlikely).
              - Non-deterministic merge algorithm due to logical error in code.
              - Data corruption in memory due to bug in code.
              - Data corruption in memory due to hardware issue.
              - Manual modification of source data after server startup.
              - Manual modification of checksums stored in ZooKeeper.
              - Part format related settings like 'enable_mixed_granularity_parts' are different on different replicas.

              Server will download merged part from replica to force byte-identical result.

        - alert: ClickHouseDistributedSyncInsertionTimeoutExceeded
          expr: increase(chi_clickhouse_event_DistributedSyncInsertionTimeoutExceeded[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased DistributedSyncInsertionTimeoutExceeded"
            description: |-
              `increase(chi_clickhouse_event_DistributedSyncInsertionTimeoutExceeded[1m])` = {{ with printf "increase(chi_clickhouse_event_DistributedSyncInsertionTimeoutExceeded{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase DistributedSyncInsertionTimeoutExceeded in `system.events` table.
              It mean Synchronous distributed insert timeout exceeded after successfull distributed connection.
              Please check documentation https://clickhouse.tech/docs/en/operations/settings/settings/#insert_distributed_sync
              And check connection between `{{ $labels.hostname }}` and all nodes in shards from remote_servers config section

        - alert: ClickHouseFileDescriptorBufferReadOrWriteFailed
          expr: increase(chi_clickhouse_event_ReadBufferFromFileDescriptorReadFailed[1m]) > 0 or increase(chi_clickhouse_event_WriteBufferFromFileDescriptorWriteFailed[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased ReadBufferFromFileDescriptorReadFailed or WriteBufferFromFileDescriptorWriteFailed"
            description: |-
              `increase(chi_clickhouse_event_ReadBufferFromFileDescriptorReadFailed[1m])` = {{ with printf "increase(chi_clickhouse_event_ReadBufferFromFileDescriptorReadFailed{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `increase(chi_clickhouse_event_WriteBufferFromFileDescriptorWriteFailed[1m])` = {{ with printf "increase(chi_clickhouse_event_WriteBufferFromFileDescriptorWriteFailed{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase ReadBufferFromFileDescriptorReadFailed or ReadBufferFromFileDescriptorReadFailed in `system.events` table.
              It mean the read (read/pread) or writes (write/pwrite) to a file descriptor. Does not include sockets.
              System can't read or write to some files.
              Please check logs on clickhouse-server pods ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- bash -c 'cat /var/log/clickhouse-server/*.err.log | grep -E "Cannot write to file|Cannot read from file"'```

        - alert: ClickHouseSlowRead
          expr: increase(chi_clickhouse_event_SlowRead[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Increased SlowRead"
            description: |-
              `increase(chi_clickhouse_event_SlowRead[1m])` = {{ with printf "increase(chi_clickhouse_event_SlowRead{hostname='%s',exported_namespace='%s'}[1m])" .Labels.hostname .Labels.exported_namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              `clickhouse-server` increase SlowRead in `system.events` table.
              It mean reads from a files that were slow. This indicate system overload. Thresholds are controlled by `SELECT * FROM system.settings WHERE name LIKE 'read_backoff_%'`.
              System will reduce the number of threads which used for processing queries.
              Check you disks utilization and hardware failures.

        - alert: ClickHouseTooMuchMutations
          expr: chi_clickhouse_table_mutations > 100
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.hostname }}"
            summary: "Too much incomplete system.mutations"
            description: |-
              `chi_clickhouse_table_mutations` = {{ with printf "chi_clickhouse_table_mutations{hostname='%s',exported_namespace='%s',database='%s',table='%s'}" .Labels.hostname .Labels.exported_namespace .Labels.database .Labels.table | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `chi_clickhouse_table_mutations_parts_to_do` = {{ with printf "chi_clickhouse_table_mutations_parts_to_do{hostname='%s',exported_namespace='%s',database='%s',table='%s'}" .Labels.hostname .Labels.exported_namespace .Labels.database .Labels.table | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `system.mutations` show too much active mutations.
              It mean something wrong with ALTER TABLE DELETE/UPDATE queries.
              Please check mutations errors ```kubectl exec -n {{ $labels.exported_namespace }} pod/$(kubectl get pods -n {{ $labels.exported_namespace }} | grep $( echo {{ $labels.hostname }} | cut -d '.' -f 1) | cut -d " " -f 1) -- clickhouse-client -q "SELECT * FROM system.mutations WHERE is_done=0 FORMAT Vertical"```
              Read about how to run KILL MUTATION
              https://clickhouse.tech/docs/en/sql-reference/statements/kill/#kill-mutation

    # based on https://blog.serverdensity.com/how-to-monitor-zookeeper/
    # and https://github.com/apache/zookeeper/blob/master/zookeeper-contrib/zookeeper-contrib-monitoring/nagios/services.cfg
    - name: ZookeeperRules
      rules:
        - alert: ZookeeperDown
          expr: up{app='zookeeper'} == 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.pod }}"
            summary: "zookeeper possible down"
            description: |-
              `zookeeper` can't be scraped via prometheus.
              Please check instance status
              ```kubectl logs -n {{ $labels.namespace }} {{ $labels.pod }} -f```

        - alert: ZookeeperRestartRecently
          expr: uptime{app='zookeeper'} > 1 < 180000
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Amount of time it takes for the server to respond to a client request (since the server was started)."
            description: |-
              `uptime{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "uptime{pod='%s',namespace='%s'} / 1000" .Labels.pod .Labels.namespace | query }}{{ . | first | value | humanizeDuration }}{{ end }}

              Look to previous Zookeeper pod log to investigate restart reason
              ```
              kubectl logs -n {{ $labels.namespace }} pod/{{ $labels.pod }} --previous
              ```

        - alert: ZookeeperHighLatency
          expr: max_latency{app='zookeeper'} > 500
          for: 60s
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Amount of time it takes for the server to respond to a client request (since the server was started)."
            description: |-
              `max_latency{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "max_latency{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} ticks{{ end }}

              Look to CPU/Memory node/pod utilization
              ```
              kubectl top -n {{ $labels.namespace }} pod {{ $labels.pod }}
              kubectl top node {{ $labels.node }}
              ```

              Look to Zookeeper Disk free space
              ```
              kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- df -h
              ```

              Look to zookeeper read\write
              ```
              readBegin=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^rchar" | cut -d " " -f 2)
              writeBegin=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^wchar" | cut -d " " -f 2)
              sleep 5
              readEnd=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^rchar" | cut -d " " -f 2)
              writeEnd=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^wchar" | cut -d " " -f 2)
              echo "Zookeeper Read $((($readEnd - $readBegin) / 5)) b/s"
              echo "Zookeeper Write $((($writeEnd - $writeBegin) / 5)) b/s"
              ```

        - alert: ZookeeperOutstandingRequests
          expr: outstanding_requests{app='zookeeper'} > 10
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper receives more requests than it can process."
            description: |-
              `outstanding_requests{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "outstanding_requests{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} ticks{{ end }}

              Look to CPU/Memory node/pod utilization
              ```
              kubectl top -n {{ $labels.namespace }} pod {{ $labels.pod }}
              kubectl top node {{ $labels.node }}
              ```

              Look to Zookeeper Disk free space
              ```
              kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- df -h
              ```

              Look to zookeeper read\write
              ```
              readBegin=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^rchar" | cut -d " " -f 2)
              writeBegin=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^wchar" | cut -d " " -f 2)
              sleep 5
              readEnd=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^rchar" | cut -d " " -f 2)
              writeEnd=$(kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/1/io | grep -E "^wchar" | cut -d " " -f 2)
              echo "Zookeeper Read $((($readEnd - $readBegin) / 5)) b/s"
              echo "Zookeeper Write $((($writeEnd - $writeBegin) / 5)) b/s"
              ```

        - alert: ZookeeperHighFileDescriptors
          expr: (open_file_descriptor_count{app='zookeeper'} / max_file_descriptor_count{app='zookeeper'})  > 0.7
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Number of file descriptors used over the limit."
            description: |-
              `open_file_descriptor_count{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "open_file_descriptor_count{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} descriptors{{ end }}
              `process_open_fds{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "process_open_fds{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} descriptors{{ end }}

        - alert: ZookeeperPendingSyncs
          expr: pending_syncs{app='zookeeper'} > 10
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Possible Zookeeper master pending syncs with followers."
            description: |-
              `pending_session_queue_size{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "pending_session_queue_size{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} sessions{{ end }}

        - alert: ZookeeperPendingSessions
          expr: pending_session_queue_size{app='zookeeper'} > 10
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Possible Zookeeper pending sessions."
            description: |-
              `pending_session_queue_size{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "pending_session_queue_size{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} sessions{{ end }}

        - alert: ZookeeperThrottleRequests
          expr: increase(request_throttle_wait_count{app='zookeeper'}[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper throttle requests"
            description: |-
              `increase(request_throttle_wait_count{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(request_throttle_wait_count{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} requests{{ end }}
              Look `requestThrottleLimit`, `requestThrottleStallTime` in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_advancedConfiguration

        - alert: ZookeeperOutstandingTLSHandshakes
          expr: outstanding_tls_handshake{app='zookeeper'} > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper receives more TLS handshake than it can process."
            description: |-
              `outstanding_tls_handshake{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "outstanding_tls_handshake{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} requests{{ end }}

        - alert: ZookeeperConnectionRejected
          expr: increase(connection_rejected{app='zookeeper'}[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper reject connection."
            description: |-
              `increase(connection_rejected{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(connection_rejected{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} requests{{ end }}
              Check connections count on Zookeeper
              ```
              kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/net/sockstat
              kubectl exec -n {{ $labels.namespace }} {{ $labels.pod }} -- cat /proc/net/sockstat6
              ```

        - alert: ZookeeperHighEphemeralNodes
          expr: ephemerals_count{app='zookeeper'} > 100
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper have too high ephemeral znodes count."
            description: |-
              `ephemerals_count{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "ephemerals_count{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} nodes{{ end }}
              Look to documentation:
              https://zookeeper.apache.org/doc/current/zookeeperOver.html#Nodes+and+ephemeral+nodes

        - alert: ZookeeperUnrecoverableErrors
          expr: increase(unrecoverable_error_count{app='zookeeper'}[1m]) > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper have unhandled Exception"
            description: |-
              `increase(unrecoverable_error_count{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "rate(unrecoverable_error_count{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} exceptions{{ end }}
              It mean Zookeeper catch some unknown Exception and will close listen socket

              Look to current and previous Zookeeper pod log to investigate restart reason
              ```
              kubectl logs -n {{ $labels.namespace }} pod/{{ $labels.pod }}
              kubectl logs -n {{ $labels.namespace }} pod/{{ $labels.pod }} --previous
              ```


        - alert: ZookeeperLowGetChildrenCacheHitRate
          expr: increase(response_packet_cache_hits{app='zookeeper'}[1m]) / (increase(response_packet_cache_misses{app='zookeeper'}[1m]) + increase(response_packet_cache_hits{app='zookeeper'}[1m]) ) < 0.3
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper have inefficient get data znodes response cache."
            description: |-
              `increase(response_packet_cache_hits{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(response_packet_cache_hits{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} hits{{ end }}
              `increase(response_packet_cache_misses{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(response_packet_cache_misses{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} misses{{ end }}
              `Get Children Cache Hit Rate` = {{ with printf "increase(response_packet_cache_hits{pod='%s',namespace='%s'}[1m]) / (increase(response_packet_cache_misses{pod='%s',namespace='%s'}[1m]) + increase(response_packet_cache_hits{pod='%s',namespace='%s'}[1m]))" .Labels.pod .Labels.namespace .Labels.pod .Labels.namespace .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }}{{ end }}
              For heavy read workloads Zookeeper try to cache response for `get data` API method for save the serialization cost on popular znodes.
              Try to tune `maxResponseCacheSize`
              Look to documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_advancedConfiguration

        - alert: ZookeeperEnsembleAuthFailures
          expr: increase(ensemble_auth_fail{app='zookeeper'}[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper authentication failures with `ensemble` scheme."
            description: |-
              `increase(ensemble_auth_fail{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(response_packet_get_children_cache_hits{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} failures{{ end }}
              Look to `ensembleAuthName` in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_authOptions

        - alert: ZookeeperHighFsyncTime
          expr: fsynctime{quantile="0.5", app='zookeeper'} > 500
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper have slow fsync to disk."
            description: |-
              `increase(fsynctime_sum{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(fsynctime_sum{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} ms{{ end }}
              `increase(fsynctime_count{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(fsynctime_count{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} count{{ end }}
              `fsynctime{quantile="0.5",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "fsynctime{quantile='0.5',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} ms{{ end }}
              It mean writes to Transactional Log (WAL) takes more time than expected. If a Zookkeper crashes it can replay the WAL to recover its previous state after restart.

        - alert: ZookeeperLargeRequestsRejected
          expr: increase(large_requests_rejected{app='zookeeper'}[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper reject some large requests."
            description: |-
              `increase(large_requests_rejected{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(large_requests_rejected{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} requests{{ end }}
              It mean Zookeeper avoid JVM allocate too much memory and runs out of usable heap and ultimately crashes.
              Look to `largeRequestMaxBytes` and `largeRequestThreshold` options in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_advancedConfiguration

        - alert: ZookeeperStaleRequestsDropped
          expr: increase(stale_requests_dropped{app='zookeeper'}[1m]) > 0
          labels:
            severity: high
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper dropped stale requests."
            description: |-
              `increase(stale_requests_dropped{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(stale_requests_dropped{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} requests{{ end }}
              `Stale request` is a request sent by a connection that is now closed, and/or a request that will have a request latency higher than the `sessionTimeout`.
              Look to `requestThrottleDropStale`, `requestStaleLatencyCheck`, `requestStaleConnectionCheck` options in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_advancedConfiguration

        - alert: ZookeeperDigestMismatch
          expr: increase(digest_mismatches_count{app='zookeeper'}[1m]) > 0
          labels:
            severity: critical
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper have inconsistent data in memory."
            description: |-
              `increase(digest_mismatches_count{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(digest_mismatches_count{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} mismatches{{ end }}
              The digest feature is added to detect the data inconsistency inside ZooKeeper when loading database from disk, catching up and following leader, its doing incrementally hash check for the DataTree based on the adHash.
              Look to `digest.enabled` options in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_advancedConfiguration

        - alert: ZookeeperDigestMismatch
          expr: increase(sessionless_connections_expired[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper close connection without session."
            description: |-
              `increase(sessionless_connections_expired{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}[1m])` = {{ with printf "increase(sessionless_connections_expired{pod='%s',namespace='%s'}[1m])" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.2f" }} sessionless connections{{ end }}
              Look to `minSessionTimeout`,`maxSessionTimeout` options in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_advancedConfiguration

        - alert: ZookeeperThreadsDeadlocked
          expr: jvm_threads_deadlocked{app='zookeeper'} > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper JVM threads Deadlock occurred."
            description: |-
              JVM Thread Deadlock means a situation where two or more JVM threads are blocked forever, waiting for each other.
              Deadlock occurs when multiple threads need the same locks but obtain them in different order.

              Look to current Zookeeper pod log to investigate Deadlock reason
              ```
              kubectl logs -n {{ $labels.namespace }} pod/{{ $labels.pod }} -f | grep -i -E "deadlock|exception"
              ```

              Also look to JVM documentation about threads state:
              https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Thread.State.html

              `jvm_threads_deadlocked{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_deadlocked{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_current{pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_current{pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_state{state="NEW",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_state{state='NEW',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_state{state="RUNNABLE",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_state{state='RUNNABLE',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_state{state="BLOCKED",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_state{state='BLOCKED',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_state{state="WAITING",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_state{state='WAITING',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_state{state="TIMED_WAITING",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_state{state='TIMED_WAITING',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}
              `jvm_threads_state{state="TERMINATED",pod="{{ $labels.pod }}",namespace="{{ $labels.namespace }}"}` = {{ with printf "jvm_threads_state{state='TERMINATED',pod='%s',namespace='%s'}" .Labels.pod .Labels.namespace | query }}{{ . | first | value | printf "%.0f" }}{{ end }}

        - alert: ZookeeperUnsuccessfulHandshakes
          expr: increase(unsuccessful_handshake{app='zookeeper'}[1m]) > 0
          labels:
            severity: warning
          annotations:
            identifier: "{{ $labels.pod }}.{{ $labels.namespace }}"
            summary: "Zookeeper Unsuccessful Handshakes occurred."
            description: |-
              Look to `ssl.*` options in documentation:
              https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_authOptions

              Look to current Zookeeper pod log to investigate unsucessfull handshake reason
              ```
              kubectl logs -n {{ $labels.namespace }} pod/{{ $labels.pod }} -f | grep -i -E "tls|ssl|auth|cert|handshake"
              ```
